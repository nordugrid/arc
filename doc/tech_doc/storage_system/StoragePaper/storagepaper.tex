%%
%% This is file `elsarticle-template-num.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% elsarticle.dtx  (with options: `numtemplate')
%% 
%% Copyright 2007, 2008 Elsevier Ltd.
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% -------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Future Generation Computing Systems}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Chelonia - distributed cloud storage}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author[UiO1,UiO2]{Jon Kerr Nilsen}
\ead{j.k.nilsen@usit.uio.no}

\author[UU1]{Salman Toor}
\ead{salman.toor@it.uu.se}

\author[NIIF]{Zsombor Nagy}
\ead{zsombor@niif.hu}

\author[UU2]{Bjarte Mohn}
\ead{bjarte.mohn@fysast.uu.se}

\address[UiO1]{University of Oslo, Dept. of Physics,  P. O. Box 1048, Blindern, N-0316 Oslo, Norway}
\address[UiO2]{University of Oslo, Center for Information Technology, P. O. Box 1059, Blindern, N-0316 Oslo, Norway}
\address[UU1]{Dept. Information Technology, Div. of Scientific Computing Uppsala University, Box 256, SE-751 05 Uppsala, Sweden}
\address[NIIF]{Institute of National Information and Infrastructure Development NIIF/HUNGARNET, Victor Hugo 18-22, H-1132 Budapest, Hungary}
\address[UU2]{Dept. of Physics and Astronomy, Div. of Nuclear and Particle Physics, Uppsala University, Box 535, SE-75121 Uppsala, Sweden}


\begin{abstract}
Nice abstract. 
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Grid, Storage, Cloud
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:introduction}
The introduction should introduce the field of research, the "problem"
we are trying to solve and briefly explain our solution (and layout of
this paper). Key words: Distributed storage, clouds, grid enabled. 

\section{System Architecture Overview}
\label{sec:systemoverview}
Blah blah here (web services, written in HED etc) and a figure showing
all the services (the usual one).
\subsection{Core Services}
\label{sec:coreservices}
Below all main services are described. ADD MORE TEXT HERE.

\subsubsection{The A-Hash}
\label{sec:theahash}
The A-Hash service is a database storing objects containing key-value
pairs. It can be deployed on multiple nodes to get a replicated
database. The Librarian service uses the A-Hash as database.

\subsubsection{The Librarian}
\label{sec:librarian}
The Librarian service manages the hierarchy and metadata of files and
collections, handling of Logical Names and monitoring of the Shepherd
services. The Librarian service is stateless, it stores all the
persistent information in the A-Hash. This makes it possible to deploy
any number of independent Librarian services to provide
high-availability and load balancing. In this case all the Librarians
should communicate with the same set of A-Hashes in order to use the
same database of metadata.

\subsubsection{The Shephard}
\label{sec:shephard}
The Shepherd service manages a particular storage node and provides
uniform interface for storing and accessing file replicas. The
Shepherd service itself cannot transfer files, so on a storage node
there must be at least one independent storage element service (with
an interface such as HTTP(S), ByteIO, etc.) which is capable of
performing the actual file transfer. So a storage node consists of a
Shepherd service and a storage element service connected
together. Storage element services can either be KnowARC or
third-party services. For each kind of storage element service, a
Shepherd backend module is needed which enables the Shepherd service
to communicate with the storage element service, e.g. to initiate file
uploads, downloads and removal, and to detect whether a file transfer
was successful or not. Currently there are three Shepherd backends:
for the ARC native HTTP(S) server called Hopi; for the Apache web
server; and for a service which implements a subset of the ByteIO
interface.

\subsubsection{The Bartender}
\label{sec:bartender}
The Bartender service provides a high-level interface of the storage
system for the clients (other services or users). The clients can
create and remove collections (directories), create, get and remove
files, move files and collections within the namespace using Logical
Names. There can be access policies associated with files and
collections, which are evaluated every time a user wants to access
them. The Bartender communicates with the Librarian and Shepherd
services to accomplish the client’s requests.  The file content
itself does not go through the Bartender; file transfers are directly
performed between the storage nodes and the clients. The Bartender
also has so-called gateway modules which make it possible to
communicate with third-party storage solutions, thus the user can
access multiple storage systems through a single Bartender
client. Currently there is only one gateway module, for third-party
storages with GridFTP 3 interface. It is possible to deploy multiple
Bartenders for high-availability and load balancing. In this case all
the Bartenders show the same namespace and work with the same files
and collections. They can use different set of Librarians as long as
all the Librarians are using the same set of A-Hashes.

\subsection{Security}
\label{sec:security}
As is the case for all openly accessible web services, the security
model is of crucial importance for the NorduGrid storage. The security
architecture of the storage can be split into three parts; the
inter-service authorization; the transfer-level authorization; and the
high-level authorization:

The communication with and within the storage system is realized
through HTTPS with standard X.509 authentication.

\subsubsection{Inter-Service Authorization}
The inter-service authorization maintains the integrity of the
internal communication between services. There are several
communication paths between the services in the storage system. The
Bartenders send requests to the Librarians and the Shepherds, the
Shepherds communicate with the Librarians and the Librarians talk with
the A-Hash. If any of these services is compromised or a new rogue
service gets inserted in the system, the security of the entire system
is compromised. To enable trust between the services, they need to
know each other's Distinguished Names (DNs). This way, a rogue service
would need to obtain a certificate with that exact DN from some
trusted Certificate Authority (CA).

\subsubsection{Transfer-Level Authorization}
The transfer-level authorization handles the authorization in the
cases of uploading and downloading files. When a transfer is
requested, the Shepherd will provide a one-time Transfer URL (TURL) to
which the client can connect. In the current architecture, this TURL
is world-accessible. This may not seem very secure at first. However,
provided that the TURL has a very long, unguessable name, that it is
transfered to the user in a secure way and that it can only be
accessed once before it is deleted, the chance of being compromised is
very low.

\subsubsection{High-Level Authorization}
The high-level authorization considers the access policies for the
files and collections in the system. These policies are stored in
A-Hash, in the metadata of the corresponding file or collection,
providing a fine-grained security in the system.

\subsection{Client Tools}
\label{sec:clienttools}
Being the only part a user will (and should) see from a storage
system, the client tools are an important part of the NorduGrid
storage. Currently ARC supports two ways of accessing the storage
solution. The \textbf{Command-line Interface} (CLI) provides access to
the storage through the methods \texttt{stat},
\texttt{makeCollection}, \texttt{unmakeCollection}, \texttt{putFile},
\texttt{getFile}, \texttt{delFile}, \texttt{list} and
\texttt{move}. Methods for modifying access and ownership will be
available in the near future.

The \textbf{FUSE module} provides a high-level access to the storage
system. Filesystem in Userspace (FUSE)~\cite{FUSE} provides a simple
library and a kernel-userspace interface. Using FUSE and the ARC
Python interface, the FUSE module allows users to mount the storage
namespace into the local namespace, enabling the use of graphical file
browsers.

It is worth mentioning that the client tools queries the storage
system through the Bartender only. Currently upload and download is
realized through HTTP(S), but there are plans to add support for other
protocols, such as SRM and GridFTP.

\section{System Characteristics}
\label{sec:systemcharateristics}
Describe here what characterizes the system. What it offers. What it
is, what it can be used for. Anything from automatic file recovery
till that the system can behave as virtual file system should be mentioned
here.

\section{System Performance}
\label{sec:systemperformance}
Describe here the results of all our tests divided into catagories
depending on if the test was a stability/usabilty test, or if it was a
performance test only. Key word: Scalability.

\section{Related Work}
\label{sec:systemperformance}
There are a number of Grid storage solutions on the market, but there
is no direct analogue to Chelonia. 

dCache \cite{} mainly differs from Chelonia in that dCache has a
centralized set of core services while Chelonia is distributed by
design. dCache is a service oriented storage system which combines
heterogeneous storage elements to collect several hundreds of
terabytes in a single namespace. Originally designed to work on a
local area network, dCache has proven to be useful also in a Grid
environment, with the NDGF (Nordic Data Grid Facility) dCache
installation as the largest example. There, the core components, such
as the metadata catalogue, indexing service and protocol doors are run
in a centralized manner, while the storage pools are
distributed. Chelonia, designed to have multiple instances of all
services running in a Grid environment, will not need a centralized
set of core services. Additionally, dCache is relatively difficult to
deploy and integrate with new applications. Being a more light-weight
and flexible storage solution, Chelonia aims more towards new user
groups, less familiar with Grid solutions.

While Chelonia can run multiple instances of every service, the Disk
Pool Manager \cite{} (DPM) needs single instances of all services but the
storage pools. Most notably DPM has a centralized data base for
storing metadata. DPM is a lightweight solution for disk storage
management. It mainly consists of a DPM server, a DPM name server to
handle the namespace, SRM and GridFTP servers, and disk servers.
While serving as a lightweight alternative to dCache, the limited
distributed services may cause scaling challenges when it comes to the
usage rates.

Scalla \cite{} differs from Chelonia in that Scalla is designed for
use on centralized clusters, while Chelonia is designed for a
distributed environment. Scalla is a widely used software suite
consisting of an xrootd server for data access and an olbd server for
building scalable xrootd clusters. Originally developed for use with
the physics analysis tool ROOT 11 , xrootd offers data access both
through the specialized xroot protocol and through other third-party
protocols. The combination of the xrootd and olbd components offers a
cluster storage designed for low latency, high bandwidth
environments. In contrast, Chelonia is optimized for high latency and
is more suitable for the Grid environment.

As opposed to Chelonia, iRODS \cite{} does not provide any storage
itself but is more an interface to other, third-party storage
systems. Based on the client-server model, iRODS provides a flexible
data grid management system. It allows uniform access to heterogeneous
storage resources over a wide area network. Its functionality, with a
uniform namespace for several Data Grid Managers and file systems, is
quite similar to the functionality offered by our Gateway
service. However, iRODS uses a database system for maintaining the
attributes and states of data and operations. This is not needed with
Chelonia’s Gateway service.

While Chelonia is designed for geographically distributed users and
data storage, Hadoop \cite{} with its file system HDFS is directed
towards physically closely grouped clusters. HDFS builds on the
master-slave architecture where a single NameNode works as a master
and responsible for the metadata where as DataNodes are used to store
the actual data. Though similar to Chelonia’s metadata service, the
NameNode cannot be replicated and may become a bottleneck in the
system. Additionally, HDFS uses non-standard protocols for
communication and security while Chelonia uses standard protocols like
HTTP(S), GridFTP and X509.

XTtreemFS \cite{} has a very different standpoint when compared to
Chelonia, in that it is designed to work only in a specific operating
system while our Chelonia is non-intrusive by design. XTreemFS is an
object-based file system designed for the Grid environment of the
operating system XTreemOS. Being object-based, XTreemFS have separated
the metadata of files and the file content, or objects in the same
fashion as Chelonia. Being a tightly integrated part of an operating
system enables XTreemFS to implement functionality very similar to
that of local file systems while being a distributed Grid
storage. However, this also enforces a specific operating system,
requiring dedicated hardware resources.

\section{Future Work}
\label{sec:futurework}
I'm not sure if this section should be here.

\section{Conclusions}
The best system ever built.


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

\begin{thebibliography}{00}

%% \bibitem{label}
%% Text of bibliographic item

\bibitem{}

\end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
